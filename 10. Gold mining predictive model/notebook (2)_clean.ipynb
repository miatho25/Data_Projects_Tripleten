{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "As a data analyst for a gold mining company, I'm developing a predictive model to optimize the gold recovery process from ore. Using data from various stages of the flotation plant, my goal is to predict recovery rates at both the rougher and final purification stages with minimal sMAPE (Symmetric Mean Absolute Percentage Error). I'll analyze how metal concentrations change through purification stages, identify and remove anomalous measurements, and compare different machine learning models to find the most accurate predictor. The final model will enable the mining company to better control the extraction process, optimize recovery rates, and ultimately increase gold production efficiency while reducing waste and operational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets start with importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.1 Load the data files\n",
    "print(\"\\n1.1 Loading data files\")\n",
    "train_df = pd.read_csv('/datasets/gold_recovery_train.csv')\n",
    "test_df = pd.read_csv('/datasets/gold_recovery_test.csv')\n",
    "full_df = pd.read_csv('/datasets/gold_recovery_full.csv')\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Full dataset shape: {full_df.shape}\")\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "display(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData types in training set:\")\n",
    "print(train_df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nBasic statistics of training data:\")\n",
    "display(train_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Check recovery calculation\n",
    "print(\"\\n1.2 Check recovery calculation\")\n",
    "\n",
    "# Recovery formula: Recovery = C*(F-T)/(F*(C-T)) * 100%\n",
    "# Where: C = concentrate, F = feed, T = tails\n",
    "\n",
    "def calculate_recovery(c, f, t):\n",
    "    \"\"\"\n",
    "    Calculate recovery using the formula: Recovery = C*(F-T)/(F*(C-T)) * 100%\n",
    "    \n",
    "    Parameters:\n",
    "    c: concentrate grade\n",
    "    f: feed grade\n",
    "    t: tails grade\n",
    "    \n",
    "    Returns:\n",
    "    recovery percentage\n",
    "    \"\"\"\n",
    "    # Handle edge cases to avoid division by zero and invalid values\n",
    "    if pd.isna(c) or pd.isna(f) or pd.isna(t):\n",
    "        return np.nan\n",
    "    \n",
    "    if f == 0 or (c - t) == 0:\n",
    "        return 0\n",
    "    \n",
    "    if c == t: \n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        recovery = (c * (f - t)) / (f * (c - t)) * 100\n",
    "        # Check if result is valid\n",
    "        if np.isfinite(recovery):\n",
    "            return recovery\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Calculate rougher output recovery\n",
    "c_rougher = train_df['rougher.output.concentrate_au'].values\n",
    "f_rougher = train_df['rougher.input.feed_au'].values\n",
    "t_rougher = train_df['rougher.output.tail_au'].values\n",
    "\n",
    "# Calculate recovery for each row\n",
    "calculated_recovery = []\n",
    "for i in range(len(train_df)):\n",
    "    recovery = calculate_recovery(c_rougher[i], f_rougher[i], t_rougher[i])\n",
    "    calculated_recovery.append(recovery)\n",
    "\n",
    "calculated_recovery = np.array(calculated_recovery)\n",
    "\n",
    "# Get actual recovery values from the dataset\n",
    "actual_recovery = train_df['rougher.output.recovery'].values\n",
    "\n",
    "# Remove NaN values for comparison\n",
    "mask = ~(np.isnan(calculated_recovery) | np.isnan(actual_recovery))\n",
    "calculated_recovery_clean = calculated_recovery[mask]\n",
    "actual_recovery_clean = actual_recovery[mask]\n",
    "\n",
    "print(f\"Total samples: {len(train_df)}\")\n",
    "print(f\"Valid samples for comparison: {len(actual_recovery_clean)}\")\n",
    "\n",
    "# Calculate MAE\n",
    "mae_recovery = mean_absolute_error(actual_recovery_clean, calculated_recovery_clean)\n",
    "print(f\"\\nMAE between calculated and actual recovery: {mae_recovery:.4f}\")\n",
    "\n",
    "# Calculated vs Actual Findings\n",
    "print(\"\\nComparison of calculated vs actual recovery Findings (first 5 valid rows):\")\n",
    "valid_indices = np.where(mask)[0][:5]\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': actual_recovery[valid_indices],\n",
    "    'Calculated': calculated_recovery[valid_indices],\n",
    "    'Difference': abs(actual_recovery[valid_indices] - calculated_recovery[valid_indices])\n",
    "})\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Analyze features not available in test set\n",
    "print(\"\\n1.3 Analyzing features not available in test set\")\n",
    "\n",
    "train_columns = set(train_df.columns)\n",
    "test_columns = set(test_df.columns)\n",
    "missing_in_test = train_columns - test_columns\n",
    "\n",
    "print(f\"\\nNumber of features in training set: {len(train_columns)}\")\n",
    "print(f\"Number of features in test set: {len(test_columns)}\")\n",
    "print(f\"Number of features missing in test set: {len(missing_in_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nFeatures missing in test set:\")\n",
    "missing_features = sorted(list(missing_in_test))\n",
    "for feature in missing_features:\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the type of missing features\n",
    "print(\"\\nAnalysis of missing features:\")\n",
    "output_features = [f for f in missing_features if 'output' in f]\n",
    "calc_features = [f for f in missing_features if 'calc' in f]\n",
    "\n",
    "print(f\"Output features (measured after process): {len(output_features)}\")\n",
    "print(f\"Calculated features: {len(calc_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Parameters Findings\n",
    "\n",
    "- What are these parameters?\n",
    "The 34 features missing in the test set fall into two main categories:\n",
    "\n",
    "1. Output Parameters\n",
    "These are measurements taken after the flotation process at various stages:\n",
    "\n",
    "Recovery rates (e.g., rougher.output.recovery, final.output.recovery)\n",
    "Concentrate measurements (e.g., final.output.concentrate_au, final.output.concentrate_ag)\n",
    "Tail measurements (e.g., rougher.output.tail_au, final.output.tail_sol)\n",
    "\n",
    "2. Calculated Parameters\n",
    "These are derived values computed from other measurements:\n",
    "\n",
    "rougher.calculation.sulfate\n",
    "rougher.calculation.floatbank8_copper_sulfate\n",
    "rougher.calculation.floatbank10_copper_sulfate\n",
    "Other flotation bank calculations\n",
    "\n",
    "- What is their type?\n",
    "All missing parameters are numerical/continuous features that represent:\n",
    "\n",
    "Percentages: Recovery rates (0-100%)\n",
    "Concentrations: Metal content measurements (g/ton or similar units)\n",
    "Flow rates: Material flow measurements\n",
    "Calculated ratios: Derived metrics from process calculations\n",
    "\n",
    "- Why are they missing?\n",
    "These parameters are not available in the test set because:\n",
    "\n",
    "Output parameters can only be measured after the process is complete\n",
    "Calculated parameters depend on the output measurements\n",
    "The test set represents the input stage where we need to predict these outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Data preprocessing\n",
    "print(\"\\n1.4 Performing data preprocessing\")\n",
    "\n",
    "# Check for missing values in training set\n",
    "print(\"\\nMissing values in training set:\")\n",
    "missing_train = train_df.isnull().sum()\n",
    "display(missing_train[missing_train > 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in training set\n",
    "print(\"\\nMissing values in test set:\")\n",
    "missing_test = test_df.isnull().sum()\n",
    "print(missing_test[missing_test > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target columns\n",
    "target_cols = ['rougher.output.recovery', 'final.output.recovery']\n",
    "\n",
    "# Remove rows where target columns have missing values (training set only)\n",
    "train_df_no_missing_targets = train_df.dropna(subset=target_cols)\n",
    "print(f\"Rows removed due to missing targets: {len(train_df) - len(train_df_no_missing_targets)}\")\n",
    "\n",
    "# Fill missing values with median (only for features, not targets)\n",
    "train_df_clean = train_df_no_missing_targets.fillna(train_df_no_missing_targets.median(numeric_only=True))\n",
    "test_df_clean = test_df.fillna(test_df.median(numeric_only=True))\n",
    "\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(f\"Training set: {train_df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Test set: {test_df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY QUESTIONS TO ANSWER - Data Preparation\n",
    "\n",
    "\n",
    "After analyzing the data preparation steps, here are three key questions and answers:\n",
    "\n",
    "1. Is the recovery calculation in the dataset accurate?\n",
    "   * Calculated MAE: 0.0000\n",
    "   * Valid samples for comparison: 14287 out of 16860 total samples\n",
    "   * The very low MAE confirms the recovery values are calculated correctly using the formula\n",
    "\n",
    "2. What features are missing in the test set and why?\n",
    "   * Total missing features: 34\n",
    "   * Output parameters (measured after process): 30 features\n",
    "   * Calculated parameters (derived from outputs): 4 features\n",
    "   * These are missing because they represent future measurements we need to predict\n",
    "\n",
    "3. How much data cleaning was required?\n",
    "   * Missing values in training set: 85 features had missing values\n",
    "   * Missing values in test set: 51 features had missing values\n",
    "   * Method used: Median imputation (replacing missing values with the middle value)\n",
    "   * After cleaning: 0 missing values in both sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid orange; padding: 10px; margin: 5px\">\n",
    "<b>  Update  </b>\n",
    "    \n",
    "This column was red, was there anything that should be corrected in this section?\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Analyze the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Metal concentrations at different stages\n",
    "print(\"\\n2.1 Analyzing metal concentrations at different purification stages\")\n",
    "\n",
    "# Define the stages and metals\n",
    "stages = ['rougher', 'primary_cleaner', 'secondary_cleaner', 'final']\n",
    "metals = ['au', 'ag', 'pb']\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, metal in enumerate(metals):\n",
    "    # Collect concentration data for each stage\n",
    "    stage_data = []\n",
    "    stage_names = []\n",
    "    \n",
    "    # Input feed\n",
    "    if f'rougher.input.feed_{metal}' in train_df_clean.columns:\n",
    "        stage_data.append(train_df_clean[f'rougher.input.feed_{metal}'].mean())\n",
    "        stage_names.append('Feed')\n",
    "    \n",
    "    # Rougher concentrate\n",
    "    if f'rougher.output.concentrate_{metal}' in train_df_clean.columns:\n",
    "        stage_data.append(train_df_clean[f'rougher.output.concentrate_{metal}'].mean())\n",
    "        stage_names.append('Rougher')\n",
    "    \n",
    "    # Primary cleaner\n",
    "    if f'primary_cleaner.output.concentrate_{metal}' in train_df_clean.columns:\n",
    "        stage_data.append(train_df_clean[f'primary_cleaner.output.concentrate_{metal}'].mean())\n",
    "        stage_names.append('Primary')\n",
    "    \n",
    "    # Secondary cleaner\n",
    "    if f'secondary_cleaner.output.concentrate_{metal}' in train_df_clean.columns:\n",
    "        stage_data.append(train_df_clean[f'secondary_cleaner.output.concentrate_{metal}'].mean())\n",
    "        stage_names.append('Secondary')\n",
    "    \n",
    "    # Final concentrate\n",
    "    if f'final.output.concentrate_{metal}' in train_df_clean.columns:\n",
    "        stage_data.append(train_df_clean[f'final.output.concentrate_{metal}'].mean())\n",
    "        stage_names.append('Final')\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(stage_names, stage_data, marker='o', linewidth=2, markersize=8)\n",
    "    axes[idx].set_title(f'{metal.upper()} Concentration by Stage')\n",
    "    axes[idx].set_xlabel('Purification Stage')\n",
    "    axes[idx].set_ylabel('Average Concentration')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for metal concentrations at different stages\n",
    "print(\"\\nCreating histograms for metal concentration distributions.\")\n",
    "\n",
    "# Define the stages to analyze\n",
    "stages_to_plot = [\n",
    "    ('rougher.input.feed_', 'Feed'),\n",
    "    ('rougher.output.concentrate_', 'Rougher Concentrate'),\n",
    "    ('primary_cleaner.output.concentrate_', 'Primary Cleaner'),\n",
    "    ('final.output.concentrate_', 'Final Concentrate')\n",
    "]\n",
    "\n",
    "# Create histograms for each metal\n",
    "for metal in metals:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "    fig.suptitle(f'{metal.upper()} Concentration Distribution Across Stages', fontsize=16)\n",
    "    \n",
    "    for idx, (stage_prefix, stage_name) in enumerate(stages_to_plot):\n",
    "        col_name = f'{stage_prefix}{metal}'\n",
    "        \n",
    "        if col_name in train_df_clean.columns:\n",
    "            # Create histogram\n",
    "            train_df_clean[col_name].hist(ax=axes[idx], bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[idx].set_title(stage_name)\n",
    "            axes[idx].set_xlabel(f'{metal.upper()} Concentration')\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add mean line\n",
    "            mean_val = train_df_clean[col_name].mean()\n",
    "            axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, \n",
    "                            label=f'Mean: {mean_val:.2f}')\n",
    "            axes[idx].legend()\n",
    "        else:\n",
    "            axes[idx].text(0.5, 0.5, 'Data not available', \n",
    "                         ha='center', va='center', transform=axes[idx].transAxes)\n",
    "            axes[idx].set_title(stage_name)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical analysis\n",
    "print(\"\\nAverage metal concentrations by stage:\")\n",
    "for metal in metals:\n",
    "    print(f\"\\n{metal.upper()}:\")\n",
    "    if f'rougher.input.feed_{metal}' in train_df_clean.columns:\n",
    "        print(f\"  Feed: {train_df_clean[f'rougher.input.feed_{metal}'].mean():.2f}\")\n",
    "    if f'rougher.output.concentrate_{metal}' in train_df_clean.columns:\n",
    "        print(f\"  Rougher: {train_df_clean[f'rougher.output.concentrate_{metal}'].mean():.2f}\")\n",
    "    if f'final.output.concentrate_{metal}' in train_df_clean.columns:\n",
    "        print(f\"  Final: {train_df_clean[f'final.output.concentrate_{metal}'].mean():.2f}\")\n",
    "\n",
    "        # Initialize feed_size_cols globally\n",
    "feed_size_cols = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.2 Compare feed particle size distributions\n",
    "print(\"\\n2.2 Comparing feed particle size distributions...\")\n",
    "\n",
    "# Debug: Let's see what columns we have\n",
    "print(\"\\nChecking available columns...\")\n",
    "all_cols = list(train_df_clean.columns)\n",
    "print(f\"Total columns: {len(all_cols)}\")\n",
    "\n",
    "# Look for any size-related columns\n",
    "size_keywords = ['size', 'particle', 'granulometry', 'mesh', 'fraction']\n",
    "potential_size_cols = []\n",
    "for col in all_cols:\n",
    "    for keyword in size_keywords:\n",
    "        if keyword in col.lower():\n",
    "            potential_size_cols.append(col)\n",
    "            break\n",
    "\n",
    "print(f\"\\nPotential size-related columns found: {len(potential_size_cols)}\")\n",
    "if len(potential_size_cols) > 0:\n",
    "    print(\"Examples:\", potential_size_cols[:5])\n",
    "\n",
    "# Get feed size columns - try different patterns\n",
    "feed_size_cols = [col for col in train_df_clean.columns if 'rougher.input.feed_size' in col]\n",
    "if len(feed_size_cols) == 0:\n",
    "    # Try alternative pattern\n",
    "    feed_size_cols = [col for col in train_df_clean.columns if 'feed_size' in col]\n",
    "if len(feed_size_cols) == 0:\n",
    "    # Try another pattern\n",
    "    feed_size_cols = [col for col in train_df_clean.columns if 'size' in col and 'input' in col]\n",
    "    \n",
    "print(f\"Feed size columns found: {len(feed_size_cols)}\")\n",
    "if len(feed_size_cols) > 0:\n",
    "    print(\"Columns:\", feed_size_cols[:5], \"...\" if len(feed_size_cols) > 5 else \"\")\n",
    "    \n",
    "    # Calculate distributions\n",
    "    train_size_dist = train_df_clean[feed_size_cols].mean()\n",
    "    test_size_dist = test_df_clean[feed_size_cols].mean()\n",
    "    \n",
    "    # Visualize distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = range(len(feed_size_cols))\n",
    "    plt.plot(x, train_size_dist.values, 'b-', label='Training Set', linewidth=2, marker='o', markersize=4)\n",
    "    plt.plot(x, test_size_dist.values, 'r--', label='Test Set', linewidth=2, marker='s', markersize=4)\n",
    "    plt.xlabel('Size Fraction Index')\n",
    "    plt.ylabel('Average Percentage')\n",
    "    plt.title('Feed Particle Size Distribution Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(x[::2])  # Show every other tick to avoid crowding\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate difference\n",
    "    size_diff = abs(train_size_dist - test_size_dist).mean()\n",
    "    print(f\"\\nAverage absolute difference between distributions: {size_diff:.4f}\")\n",
    "    print(\"Note: Small differences indicate similar distributions, which is good for model reliability.\")\n",
    "    \n",
    "    # Show detailed statistics\n",
    "    print(\"\\nDistribution statistics:\")\n",
    "    print(f\"Training set - Min: {train_size_dist.min():.2f}, Max: {train_size_dist.max():.2f}, Mean: {train_size_dist.mean():.2f}\")\n",
    "    print(f\"Test set - Min: {test_size_dist.min():.2f}, Max: {test_size_dist.max():.2f}, Mean: {test_size_dist.mean():.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo feed size columns found in the data.\")\n",
    "    print(\"Checking column names for size-related features...\")\n",
    "    size_related = [col for col in train_df_clean.columns if 'size' in col.lower()]\n",
    "    print(f\"Found {len(size_related)} size-related columns:\")\n",
    "    for col in size_related[:10]:  # Show first 10\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Set a default value for size_diff to avoid errors later\n",
    "    size_diff = 0.0\n",
    "    print(\"\\nSkipping particle size distribution comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overlaid histograms for better distribution comparison\n",
    "if len(feed_size_cols) == 1:\n",
    "    # Single column - create one histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train_df_clean[feed_size_cols[0]], bins=50, alpha=0.6, label='Training Set', \n",
    "                color='blue', edgecolor='black')\n",
    "    plt.hist(test_df_clean[feed_size_cols[0]], bins=50, alpha=0.6, label='Test Set', \n",
    "                color='red', edgecolor='black')\n",
    "    plt.xlabel('Feed Size Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Feed Particle Size Distribution - Histogram Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    # Multiple columns - create subplots\n",
    "    n_cols = min(len(feed_size_cols), 4)  # Show max 4 columns per row\n",
    "    n_rows = (len(feed_size_cols) + n_cols - 1) // n_cols\n",
    "        \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "        \n",
    "    for idx, col in enumerate(feed_size_cols[:n_rows*n_cols]):\n",
    "        ax = axes[idx]\n",
    "        # Overlay histograms\n",
    "        ax.hist(train_df_clean[col], bins=30, alpha=0.6, label='Training', \n",
    "                color='blue', edgecolor='black')\n",
    "        ax.hist(test_df_clean[col], bins=30, alpha=0.6, label='Test', \n",
    "                color='red', edgecolor='black')\n",
    "        ax.set_title(f'Size Fraction {idx}')\n",
    "        ax.set_xlabel('Percentage')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(feed_size_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "        \n",
    "    plt.suptitle('Feed Particle Size Distributions - Histogram Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Analyzing total concentrations at different stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Total concentrations analysis\n",
    "\n",
    "# Calculate total concentrations for different stages\n",
    "def calculate_total_concentration(df, stage_prefix):\n",
    "    \"\"\"Calculate total concentration for a given stage\"\"\"\n",
    "    # Get all concentration columns for the stage\n",
    "    cols = [col for col in df.columns if stage_prefix in col and \n",
    "            any(metal in col for metal in ['_au', '_ag', '_pb', '_sol'])]\n",
    "    \n",
    "    if len(cols) > 0:\n",
    "        return df[cols].sum(axis=1)\n",
    "    else:\n",
    "        return pd.Series([0] * len(df))\n",
    "\n",
    "# Calculate totals for different stages\n",
    "train_df_clean['total_feed'] = calculate_total_concentration(\n",
    "    train_df_clean, 'rougher.input.feed'\n",
    ")\n",
    "train_df_clean['total_rougher'] = calculate_total_concentration(\n",
    "    train_df_clean, 'rougher.output.concentrate'\n",
    ")\n",
    "train_df_clean['total_final'] = calculate_total_concentration(\n",
    "    train_df_clean, 'final.output.concentrate'\n",
    ")\n",
    "print(\"\\n2.3 Total concentrations analyzed at different stages!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "stages_to_plot = [\n",
    "    ('total_feed', 'Raw Feed'),\n",
    "    ('total_rougher', 'Rougher Concentrate'),\n",
    "    ('total_final', 'Final Concentrate')\n",
    "]\n",
    "\n",
    "for idx, (col, title) in enumerate(stages_to_plot):\n",
    "    data = train_df_clean[col]\n",
    "    axes[idx].hist(data, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'{title} Total Concentration')\n",
    "    axes[idx].set_xlabel('Total Concentration')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.2f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for anomalies (values > 100 or < 0)\n",
    "print(\"\\nChecking for anomalous total concentrations\")\n",
    "for col, title in stages_to_plot:\n",
    "    anomalies = train_df_clean[(train_df_clean[col] > 100) | (train_df_clean[col] < 0)]\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(f\"  Total samples: {len(train_df_clean)}\")\n",
    "    print(f\"  Anomalous samples (>100 or <0): {len(anomalies)}\")\n",
    "    print(f\"  Percentage: {len(anomalies)/len(train_df_clean)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anomalies\n",
    "print(\"\\nRemoving anomalous samples\")\n",
    "initial_size = len(train_df_clean)\n",
    "\n",
    "\n",
    "# Keep only samples where total concentrations are reasonable (0-100)\n",
    "mask = (\n",
    "    (train_df_clean['total_feed'] >= 2) & (train_df_clean['total_feed'] <= 100) &\n",
    "    (train_df_clean['total_rougher'] >= 2) & (train_df_clean['total_rougher'] <= 100) &\n",
    "    (train_df_clean['total_final'] >= 2) & (train_df_clean['total_final'] <= 100)\n",
    ")\n",
    "train_df_final = train_df_clean[mask].copy()\n",
    "\n",
    "print(f\"Samples before cleaning: {initial_size}\")\n",
    "print(f\"Samples after cleaning: {len(train_df_final)}\")\n",
    "print(f\"Samples removed: {initial_size - len(train_df_final)} ({(initial_size - len(train_df_final))/initial_size*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY QUESTIONS TO ANSWER - Data Analysis\n",
    "\n",
    "\n",
    "After analyzing the data patterns, here are three key questions and answers:\n",
    "\n",
    "1. How do metal concentrations change through the purification process?\n",
    "   * Gold (Au): Feed: 7.17 → Final: 39.49 (increase of 5.5x)\n",
    "   * Silver (Ag): Feed: 7.83 → Final: 4.72 (increase of 0.6x)\n",
    "   * Lead (Pb): Feed: 3.22 → Final: 9.12 (decrease of -183.2%)\n",
    "   * Gold shows the most significant concentration increase\n",
    "\n",
    "2. Are the particle sizes similar between training and test sets?\n",
    "   * Difference in average feed size: 2.6481\n",
    "   * This small difference indicates similar particle sizes\n",
    "   * Similar sizes mean our model should work reliably on test data\n",
    "\n",
    "3. How many anomalous samples were found and removed?\n",
    "   * Samples with impossible concentrations (>100% or <0%): 0\n",
    "   * Percentage of data removed: 0.00%\n",
    "   * Final clean dataset size: 16860 samples\n",
    "   * Removing these improves model reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "\n",
    "# 3.1 Define sMAPE function\n",
    "print(\"\\n3.1 Defining sMAPE evaluation metric\")\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (sMAPE)\n",
    "    \n",
    "    Formula: sMAPE = (1/n) * Σ(|y_true - y_pred| / ((|y_true| + |y_pred|) / 2)) * 100\n",
    "    \"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    # Avoid division by zero\n",
    "    mask = denominator != 0\n",
    "    smape_value = np.zeros_like(y_true)\n",
    "    smape_value[mask] = np.abs(y_true[mask] - y_pred[mask]) / denominator[mask]\n",
    "    return 100.0 * smape_value.mean()\n",
    "\n",
    "def final_smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate final sMAPE as weighted average of two targets\n",
    "    \n",
    "    Weights: 25% for rougher recovery, 75% for final recovery\n",
    "    \"\"\"\n",
    "    # Assuming y_true and y_pred have two columns: [rougher_recovery, final_recovery]\n",
    "    if len(y_true.shape) == 1:  # Single target\n",
    "        return smape(y_true, y_pred)\n",
    "    \n",
    "    smape_rougher = smape(y_true[:, 0], y_pred[:, 0])\n",
    "    smape_final = smape(y_true[:, 1], y_pred[:, 1])\n",
    "    \n",
    "    return 0.25 * smape_rougher + 0.75 * smape_final\n",
    "\n",
    "# Create scorer for sklearn\n",
    "smape_scorer = make_scorer(smape, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.2 Train Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "\n",
    "# Define target columns\n",
    "target_cols = ['rougher.output.recovery', 'final.output.recovery']\n",
    "# Check if target columns exist\n",
    "print(f\"\\nChecking for required columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check if train_df_final exists\n",
    "if 'train_df_final' not in locals():\n",
    "    print(\"ERROR: train_df_final doesn't exist! Cannot proceed.\")\n",
    "    print(\"Check if anomaly removal in step 2.3 removed all data.\")\n",
    "    # Create empty variables to prevent later errors\n",
    "    X_train_scaled = np.array([])\n",
    "    y_train_rougher = pd.Series([])\n",
    "    y_train_final = pd.Series([])\n",
    "    X_test_scaled = np.array([])\n",
    "else:\n",
    "    print(f\" train_df_final exists with {len(train_df_final)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for target columns\n",
    "missing_targets = []\n",
    "for col in target_cols:\n",
    "    if col in train_df_final.columns:\n",
    "        print(f\" {col} found\")\n",
    "    else:\n",
    "        print(f\" {col} NOT FOUND\")\n",
    "        missing_targets.append(col)\n",
    "    \n",
    "if len(missing_targets) > 0:\n",
    "    print(f\"\\nERROR: Missing target columns: {missing_targets}\")\n",
    "    print(\"Cannot proceed with model training without target variables!\")\n",
    "    # Create empty variables\n",
    "    X_train_scaled = np.array([])\n",
    "    y_train_rougher = pd.Series([])\n",
    "    y_train_final = pd.Series([])\n",
    "    X_test_scaled = np.array([])\n",
    "else:\n",
    "    # Get feature columns (exclude targets and columns not in test set)\n",
    "    feature_cols = [col for col in train_df_final.columns \n",
    "                    if col in test_df_clean.columns and col != 'date']\n",
    "        \n",
    "    print(f\"\\nNumber of features: {len(feature_cols)}\")\n",
    "    print(f\"Target variables: {target_cols}\")\n",
    "        \n",
    "    # Show some feature examples\n",
    "    print(f\"\\nExample features: {feature_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "try:\n",
    "    X_train = train_df_final[feature_cols]\n",
    "    y_train_rougher = train_df_final[target_cols[0]]\n",
    "    y_train_final = train_df_final[target_cols[1]]\n",
    "    X_test = test_df_clean[feature_cols]\n",
    "            \n",
    "    print(f\"\\nData shapes before scaling:\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  y_train_rougher: {y_train_rougher.shape}\")\n",
    "    print(f\"  y_train_final: {y_train_final.shape}\")\n",
    "    print(f\"  X_test: {X_test.shape}\")\n",
    "            \n",
    "    # Scale features\n",
    "    print(\"\\nScaling features:\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "    print(\"Feature scaling completed successfully\")\n",
    "    print(f\"\\nFinal data shapes:\")\n",
    "    print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "    print(f\"  X_test_scaled: {X_test_scaled.shape}\")\n",
    "            \n",
    "    print(\"\\nData preparation complete!\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during data preparation: {str(e)}\")\n",
    "    print(\"Creating empty variables to continue\")\n",
    "    X_train_scaled = np.array([])\n",
    "    y_train_rougher = pd.Series([])\n",
    "    y_train_final = pd.Series([])\n",
    "    X_test_scaled = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"DATA PREPARATION SUMMARY\")\n",
    "\n",
    "print(f\"X_train_scaled exists: {'X_train_scaled' in locals() and len(X_train_scaled) > 0}\")\n",
    "print(f\"y_train_rougher exists: {'y_train_rougher' in locals() and len(y_train_rougher) > 0}\")\n",
    "print(f\"y_train_final exists: {'y_train_final' in locals() and len(y_train_final) > 0}\")\n",
    "print(f\"X_test_scaled exists: {'X_test_scaled' in locals() and len(X_test_scaled) > 0}\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.3 Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Train and evaluate models (This is step 3.2 in the instructions)\n",
    "print(\"\\n3.3 Training and evaluating models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import required library\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=5),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "print(f\"Models to evaluate: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the necessary data\n",
    "required_vars = {\n",
    "    'X_train_scaled': 'X_train_scaled' in locals() and len(X_train_scaled) > 0,\n",
    "    'y_train_rougher': 'y_train_rougher' in locals() and len(y_train_rougher) > 0,\n",
    "    'y_train_final': 'y_train_final' in locals() and len(y_train_final) > 0,\n",
    "    'X_test_scaled': 'X_test_scaled' in locals() and len(X_test_scaled) > 0\n",
    "}\n",
    "\n",
    "print(\"Checking required variables:\")\n",
    "for var_name, exists in required_vars.items():\n",
    "    print(f\"  {var_name}: {' Found' if exists else ' Not found'}\")\n",
    "\n",
    "if not all(required_vars.values()):\n",
    "    print(\"\\nERROR: Missing required data for model training!\")\n",
    "    print(\"Cannot proceed - check previous steps.\")\n",
    "else:\n",
    "    print(f\"\\n All required data found!\")\n",
    "    print(f\"Training data: {X_train_scaled.shape[0]} samples, {X_train_scaled.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Only run if we have all required data\n",
    "if all(required_vars.values()):\n",
    "    # Train and evaluate each model\n",
    "    for model_num, (name, model) in enumerate(models.items(), 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Model {model_num}/{len(models)}: {name}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validate for rougher recovery\n",
    "            print(f\"Cross-validating for rougher recovery...\")\n",
    "            cv_scores_rougher = cross_val_score(\n",
    "                model, X_train_scaled, y_train_rougher, \n",
    "                cv=3, scoring=smape_scorer  # Reduced from 5 to 3 folds\n",
    "            )\n",
    "            rougher_smape = -cv_scores_rougher.mean()\n",
    "            \n",
    "            # Cross-validate for final recovery\n",
    "            print(f\"Cross-validating for final recovery...\")\n",
    "            cv_scores_final = cross_val_score(\n",
    "                model, X_train_scaled, y_train_final, \n",
    "                cv=3, scoring=smape_scorer  # Reduced from 5 to 3 folds\n",
    "            )\n",
    "            final_smape = -cv_scores_final.mean()\n",
    "            \n",
    "            # Calculate weighted total\n",
    "            total_smape = 0.25 * rougher_smape + 0.75 * final_smape\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'rougher_smape': rougher_smape,\n",
    "                'final_smape': final_smape,\n",
    "                'total_smape': total_smape\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Rougher Recovery sMAPE: {rougher_smape:.4f}\")\n",
    "            print(f\"  Final Recovery sMAPE: {final_smape:.4f}\")\n",
    "            print(f\"  Total sMAPE (weighted): {total_smape:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR training {name}: {str(e)}\")\n",
    "else:\n",
    "    print(\"Skipping model training due to missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Display summary\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    \n",
    "    if len(results) > 0:\n",
    "        # Create summary DataFrame for easy viewing\n",
    "        summary_df = pd.DataFrame(results).T\n",
    "        print(summary_df.round(4))\n",
    "        \n",
    "        # Select best model\n",
    "        best_model_name = min(results, key=lambda x: results[x]['total_smape'])\n",
    "        print(f\"\\nBest model: {best_model_name}\")\n",
    "        print(f\"Best total sMAPE: {results[best_model_name]['total_smape']:.4f}\")\n",
    "        \n",
    "        # Train best model on full data\n",
    "        print(f\"\\nTraining {best_model_name} on full training data\")\n",
    "        best_model = models[best_model_name]\n",
    "        \n",
    "        # Train separate models for each target\n",
    "        best_model_rougher = best_model.__class__(**best_model.get_params())\n",
    "        best_model_final = best_model.__class__(**best_model.get_params())\n",
    "        \n",
    "        best_model_rougher.fit(X_train_scaled, y_train_rougher)\n",
    "        best_model_final.fit(X_train_scaled, y_train_final)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        print(\"\\nMaking predictions on test set...\")\n",
    "        test_pred_rougher = best_model_rougher.predict(X_test_scaled)\n",
    "        test_pred_final = best_model_final.predict(X_test_scaled)\n",
    "        \n",
    "        print(f\" Predictions completed\")\n",
    "        print(f\"  Test predictions shape: ({len(test_pred_rougher)},)\")\n",
    "    else:\n",
    "        print(\"No models were successfully trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model on full data and make predictions\n",
    "if len(results) > 0 and all(required_vars.values()):\n",
    "    print(f\"\\nTraining {best_model_name} on full training data\")\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    try:\n",
    "        # Train separate models for each target\n",
    "        best_model_rougher = best_model.__class__(**best_model.get_params())\n",
    "        best_model_final = best_model.__class__(**best_model.get_params())\n",
    "        \n",
    "        best_model_rougher.fit(X_train_scaled, y_train_rougher)\n",
    "        best_model_final.fit(X_train_scaled, y_train_final)\n",
    "        print(\"Models trained successfully\")\n",
    "        \n",
    "        # Show model details\n",
    "        print(f\"\\nModel details:\")\n",
    "        print(f\"  Model type: {best_model.__class__.__name__}\")\n",
    "        if hasattr(best_model, 'get_params'):\n",
    "            params = best_model.get_params()\n",
    "            print(f\"  Parameters: {params}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR training final models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "if 'best_model_rougher' in locals() and 'best_model_final' in locals():\n",
    "    try:\n",
    "        print(\"\\nMaking predictions on test set\")\n",
    "        test_pred_rougher = best_model_rougher.predict(X_test_scaled)\n",
    "        test_pred_final = best_model_final.predict(X_test_scaled)\n",
    "        \n",
    "        print(f\" Predictions completed\")\n",
    "        print(f\"  Rougher predictions: {test_pred_rougher.shape}\")\n",
    "        print(f\"  Final predictions: {test_pred_final.shape}\")\n",
    "        print(f\"\\nPrediction statistics:\")\n",
    "        print(f\"  Rougher - Mean: {test_pred_rougher.mean():.2f}, Std: {test_pred_rougher.std():.2f}\")\n",
    "        print(f\"  Final - Mean: {test_pred_final.mean():.2f}, Std: {test_pred_final.std():.2f}\")\n",
    "        print(f\"\\nSample predictions (first 5):\")\n",
    "        print(f\"  Rougher: {test_pred_rougher[:5].round(2)}\")\n",
    "        print(f\"  Final: {test_pred_final[:5].round(2)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR making predictions: {str(e)}\")\n",
    "else:\n",
    "    print(\"Best models not available for predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "if len(results) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create bar plot of model performance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    models_names = list(results.keys())\n",
    "    rougher_scores = [results[m]['rougher_smape'] for m in models_names]\n",
    "    final_scores = [results[m]['final_smape'] for m in models_names]\n",
    "    total_scores = [results[m]['total_smape'] for m in models_names]\n",
    "    \n",
    "    x = np.arange(len(models_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, rougher_scores, width, label='Rougher sMAPE', alpha=0.8)\n",
    "    plt.bar(x, final_scores, width, label='Final sMAPE', alpha=0.8)\n",
    "    plt.bar(x + width, total_scores, width, label='Total sMAPE', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('sMAPE (%)')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(x, models_names)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(total_scores):\n",
    "        plt.text(i + width, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY QUESTIONS TO ANSWER - Model Building\n",
    "============================================================\n",
    "\n",
    "After building and evaluating models, here are three key questions and answers:\n",
    "\n",
    "1. Which model performed best and what was its sMAPE score?\n",
    "   * Best model: Random Forest\n",
    "   * Total sMAPE: 11.6550 (lower is better)\n",
    "   * This combines 25% rougher recovery + 75% final recovery predictions\n",
    "\n",
    "2. How do the three models compare in performance?\n",
    "   * Linear Regression total sMAPE: 12.5590\n",
    "   * Decision Tree total sMAPE: 11.7042\n",
    "   * Random Forest total sMAPE: 11.6550\n",
    "   * Random Forest shows the best balance of accuracy\n",
    "\n",
    "3. Which recovery prediction is more challenging?\n",
    "   * Average rougher recovery sMAPE across models: 11.2812\n",
    "   * Average final recovery sMAPE across models: 12.2032\n",
    "   * Final recovery is harder to predict, showing higher error rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PROJECT SUMMARY AND FINDINGS*\n",
    "==================================================\n",
    "\n",
    "1. DATA PREPARATION FINDINGS:\n",
    "   - MAE for recovery calculation: 0.0000 (close to 0 indicates correct calculation)\n",
    "   - Valid samples for recovery comparison: 14287 out of 16860\n",
    "   - Missing features in test set: 34 (mostly output and calculated features)\n",
    "   - Anomalous samples removed: 0 (0.00%)\n",
    "\n",
    "2. DATA ANALYSIS FINDINGS:\n",
    "   - Metal concentrations increase through purification stages\n",
    "   - Gold (Au) shows the most significant concentration increase\n",
    "   - Feed particle size distributions are similar (avg diff: 2.6481)\n",
    "   - Some samples had unrealistic total concentrations (>100%) and were removed\n",
    "\n",
    "3. MODEL PERFORMANCE:\n",
    "\n",
    "   Linear Regression:\n",
    "     - Rougher sMAPE: 11.2152\n",
    "     - Final sMAPE: 13.0069\n",
    "     - Total sMAPE: 12.5590\n",
    "\n",
    "   Decision Tree:\n",
    "     - Rougher sMAPE: 11.3309\n",
    "     - Final sMAPE: 11.8286\n",
    "     - Total sMAPE: 11.7042\n",
    "\n",
    "   Random Forest:\n",
    "     - Rougher sMAPE: 11.2975\n",
    "     - Final sMAPE: 11.7742\n",
    "     - Total sMAPE: 11.6550\n",
    "\n",
    "   Best Model: Random Forest with total sMAPE of 11.6550\n",
    "\n",
    "4. RECOMMENDATIONS:\n",
    "   - The model performs reasonably well with the given features\n",
    "   - Random Forest typically performs best due to its ability to capture non-linear relationships\n",
    "   - Further improvements could include:\n",
    "     * Feature engineering (ratios, differences between stages)\n",
    "     * More sophisticated anomaly detection\n",
    "     * Ensemble methods combining multiple models\n",
    "     * Hyperparameter tuning for optimal performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
