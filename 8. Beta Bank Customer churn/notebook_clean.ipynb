{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "As a data scientist for Beta Bank, I'm addressing customer churn by developing a predictive model to identify customers likely to leave the bank. Using customer data including demographics, account details, and banking behavior, my goal is to build a model with an F1 score of at least 0.59. I'll process the data, handle class imbalance using multiple approaches, and compare different models to find the best performer. The final model will enable Beta Bank to implement targeted retention strategies, reducing customer attrition and saving costs associated with customer acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Data loading \n",
    "First, lets import all the libraries we will need for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('/datasets/Churn.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nData info:\")\n",
    "print(data.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(data.describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Data Preparation\n",
    "Now let's prepare the data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightblue; color:darkblue\">\n",
    "\n",
    "I added charts below, along with conclusions and duplication checks. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['RowNumber', 'CustomerId', 'Surname']\n",
    "existing_columns = [col for col in columns_to_drop if col in data.columns]\n",
    "data = data.drop(existing_columns, axis=1)\n",
    "print(f\"Dropped columns: {existing_columns}\")\n",
    "\n",
    "# **NEW: Check for duplicates after removing identifier columns**\n",
    "print(f\"\\nChecking for duplicates after removing identifier columns:\")\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "   print(\"Removing duplicate rows...\")\n",
    "   data = data.drop_duplicates()\n",
    "   print(f\"Dataset shape after removing duplicates: {data.shape}\")\n",
    "else:\n",
    "   print(\"No duplicate rows found.\")\n",
    "\n",
    "# **NEW: Data Distribution Analysis with Visualizations**\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle('Distribution Analysis of Key Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Analyze numerical features\n",
    "numerical_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary', 'NumOfProducts']\n",
    "\n",
    "# 1. Credit Score Distribution\n",
    "axes[0,0].hist(data['CreditScore'], bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Credit Score Distribution', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Credit Score')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Age Distribution  \n",
    "axes[0,1].hist(data['Age'], bins=25, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('Customer Age Distribution', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Age (years)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Tenure Distribution\n",
    "axes[0,2].hist(data['Tenure'].dropna(), bins=11, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "axes[0,2].set_title('Customer Tenure Distribution', fontweight='bold')\n",
    "axes[0,2].set_xlabel('Tenure (years)')\n",
    "axes[0,2].set_ylabel('Frequency')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Balance Distribution (with log scale due to many zeros)\n",
    "axes[1,0].hist(data['Balance'], bins=50, color='gold', alpha=0.7, edgecolor='black')\n",
    "axes[1,0].set_title('Account Balance Distribution', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Balance ($)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Estimated Salary Distribution\n",
    "axes[1,1].hist(data['EstimatedSalary'], bins=30, color='plum', alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_title('Estimated Salary Distribution', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Estimated Salary ($)')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Number of Products\n",
    "product_counts = data['NumOfProducts'].value_counts().sort_index()\n",
    "axes[1,2].bar(product_counts.index, product_counts.values, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1,2].set_title('Number of Banking Products', fontweight='bold')\n",
    "axes[1,2].set_xlabel('Number of Products')\n",
    "axes[1,2].set_ylabel('Number of Customers')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Geography Distribution\n",
    "geo_counts = data['Geography'].value_counts()\n",
    "axes[2,0].bar(geo_counts.index, geo_counts.values, color=['#ff9999', '#66b3ff', '#99ff99'], alpha=0.7, edgecolor='black')\n",
    "axes[2,0].set_title('Customer Geography Distribution', fontweight='bold')\n",
    "axes[2,0].set_xlabel('Country')\n",
    "axes[2,0].set_ylabel('Number of Customers')\n",
    "axes[2,0].tick_params(axis='x', rotation=45)\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Gender Distribution\n",
    "gender_counts = data['Gender'].value_counts()\n",
    "axes[2,1].pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%', \n",
    "             colors=['#ff9999', '#66b3ff'], startangle=90)\n",
    "axes[2,1].set_title('Customer Gender Distribution', fontweight='bold')\n",
    "\n",
    "# 9. Box plot for detecting outliers in key numerical features\n",
    "data_for_boxplot = data[['Age', 'CreditScore', 'Balance', 'EstimatedSalary']].copy()\n",
    "# Normalize for better visualization\n",
    "for col in data_for_boxplot.columns:\n",
    "   data_for_boxplot[col] = (data_for_boxplot[col] - data_for_boxplot[col].mean()) / data_for_boxplot[col].std()\n",
    "\n",
    "axes[2,2].boxplot([data_for_boxplot[col].dropna() for col in data_for_boxplot.columns], \n",
    "                 labels=data_for_boxplot.columns)\n",
    "axes[2,2].set_title('Outlier Detection (Standardized)', fontweight='bold')\n",
    "axes[2,2].set_ylabel('Standardized Values')\n",
    "axes[2,2].tick_params(axis='x', rotation=45)\n",
    "axes[2,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# **Conclusions from Distribution Analysis**\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KEY INSIGHTS FROM DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "CREDIT SCORE: \n",
    "  - Approximately normal distribution centered around 650\n",
    "  - Range: 350-850, which is typical for credit scores\n",
    "  - No significant outliers detected\n",
    "\n",
    "AGE:\n",
    "  - Right-skewed distribution with most customers aged 25-45\n",
    "  - Few elderly customers (>65), suggesting target demographic\n",
    "  - Some potential outliers in the 80+ range\n",
    "\n",
    "TENURE:\n",
    "  - Relatively uniform distribution from 0-10 years\n",
    "  - Missing values present (909 missing) - will need imputation\n",
    "  - Peak at lower tenure values suggests recent customer acquisition\n",
    "\n",
    "BALANCE:\n",
    "  - Heavily right-skewed with many zero balances\n",
    "  - Large concentration of customers with $0 balance\n",
    "  - High-value outliers present (>$200K) but appear legitimate\n",
    "\n",
    "ESTIMATED SALARY:\n",
    "  - Approximately uniform distribution from $11K to $200K\n",
    "  - Good spread across income levels\n",
    "  - No significant outliers detected\n",
    "\n",
    "PRODUCTS:\n",
    "  - Most customers have 1-2 banking products\n",
    "  - Very few customers have 3-4 products\n",
    "  - Potential churn indicator: customers with 1 product may be less engaged\n",
    "\n",
    "GEOGRAPHY:\n",
    "  - France has the highest customer base (~50%)\n",
    "  - Germany and Spain are roughly equal (~25% each)\n",
    "  - Geographic distribution should be considered in modeling\n",
    "\n",
    "GENDER:\n",
    "  - Relatively balanced gender distribution (slight female majority)\n",
    "  - No significant gender bias in the dataset\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['RowNumber', 'CustomerId', 'Surname']\n",
    "existing_columns = [col for col in columns_to_drop if col in data.columns]\n",
    "data = data.drop(existing_columns, axis=1)\n",
    "print(f\"Dropped columns: {existing_columns}\")\n",
    "\n",
    "# Check for categorical features and encode them\n",
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nCategorical features:\", categorical_features)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
    "print(\"\\nEncoded data shape:\", data_encoded.shape)\n",
    "print(data_encoded.columns)\n",
    "\n",
    "# Split into features and target\n",
    "X = data_encoded.drop('Exited', axis=1)\n",
    "y = data_encoded['Exited']\n",
    "\n",
    "# Split the data into training, validation, and test sets (60%, 20%, 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"\\nTraining set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# Scale numerical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create copies of the datasets\n",
    "X_train = X_train.copy()\n",
    "X_val = X_val.copy() \n",
    "X_test = X_test.copy()\n",
    "\n",
    "# Fit scaler on training data and transform all datasets\n",
    "scaler.fit(X_train[numeric_features])\n",
    "X_train.loc[:, numeric_features] = scaler.transform(X_train[numeric_features])\n",
    "X_val.loc[:, numeric_features] = scaler.transform(X_val[numeric_features])\n",
    "X_test.loc[:, numeric_features] = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Class Balance Analysis\n",
    "Let's examine the balance of classes in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "print(\"Class distribution percentage:\")\n",
    "print(y.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Class Distribution (0: Stayed, 1: Exited)')\n",
    "plt.xlabel('Exited')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Questions to Answer\n",
    "\n",
    "After analyzing the data preparation and class balance, here are three key questions and answers we can derive from our analysis:\n",
    "\n",
    "# 1. What categorical features need to be encoded in this dataset?\n",
    "   \n",
    "The dataset contains two categorical features: 'Geography' (customer's country of residence) and 'Gender'. These were properly one-hot encoded with drop_first=True to avoid multicollinearity.\n",
    "\n",
    "# 2. Is there a class imbalance in the target variable 'Exited'?\n",
    "   \n",
    "Yes, there is a significant class imbalance. Based on the value counts, approximately 20% of customers have exited the bank (class 1), while about 80% have stayed (class 0). This imbalance will need to be addressed to build an effective predictive model.\n",
    "\n",
    "# 3. What proportion of the data has been allocated for training, validation, and testing?\n",
    "\n",
    "The data has been split into three subsets: 60% for training, 20% for validation, and 20% for testing. This allocation provides sufficient data for model training while reserving adequate independent samples for validation and final testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step. 3 Baseline Model Without Addressing Class Imbalance\n",
    "\n",
    "Let's train a baseline model without addressing the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First lets check for missing values\n",
    "# Check for missing values in the data\n",
    "print(\"Missing values in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "# Check for infinity values or extremely large values\n",
    "print(\"\\nInfinity or extremely large values in X_train:\")\n",
    "print(np.isinf(X_train).sum())\n",
    "print(\"\\nMin and max values in X_train:\")\n",
    "print(X_train.min())\n",
    "print(X_train.max())\n",
    "\n",
    "# Handle missing or invalid values\n",
    "# 1. Fill missing values\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_val = X_val.fillna(X_train.mean())  # Use training mean to avoid data leakage\n",
    "X_test = X_test.fillna(X_train.mean())\n",
    "\n",
    "# 2. Replace infinities with large finite values\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.finfo(np.float32).max)\n",
    "X_val = X_val.replace([np.inf, -np.inf], np.finfo(np.float32).max)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.finfo(np.float32).max)\n",
    "\n",
    "# Verify the fix\n",
    "print(\"\\nAfter fixing, any missing values in X_train?\")\n",
    "print(X_train.isnull().sum().sum())\n",
    "print(\"After fixing, any infinity values in X_train?\")\n",
    "print(np.isinf(X_train).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a baseline Random Forest model\n",
    "baseline_model = RandomForestClassifier(random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = baseline_model.predict(X_val)\n",
    "baseline_f1 = f1_score(y_val, y_val_pred)\n",
    "baseline_auc = roc_auc_score(y_val, baseline_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(\"\\nBaseline model results (without addressing imbalance):\")\n",
    "print(f\"F1 Score: {baseline_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {baseline_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Improving Model Quality by Addressing Class Imbalance\n",
    "We will not look at the differenet approaches to Class Imbalance.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Class Weights:\n",
    "\n",
    "# Train a Random Forest model with class weights\n",
    "weighted_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "weighted_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_weighted = weighted_model.predict(X_val)\n",
    "weighted_f1 = f1_score(y_val, y_val_pred_weighted)\n",
    "weighted_auc = roc_auc_score(y_val, weighted_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(\"\\nWeighted model results:\")\n",
    "print(f\"F1 Score: {weighted_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {weighted_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Upsampling the Minority Class\n",
    "\n",
    "# First, separate majority and minority classes\n",
    "X_train_majority = X_train[y_train == 0]\n",
    "X_train_minority = X_train[y_train == 1]\n",
    "y_train_majority = y_train[y_train == 0]\n",
    "y_train_minority = y_train[y_train == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "X_train_minority_upsampled, y_train_minority_upsampled = resample(\n",
    "    X_train_minority, \n",
    "    y_train_minority,\n",
    "    replace=True,\n",
    "    n_samples=len(X_train_majority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "X_train_upsampled = pd.concat([X_train_majority, X_train_minority_upsampled])\n",
    "y_train_upsampled = pd.concat([y_train_majority, y_train_minority_upsampled])\n",
    "\n",
    "print(\"\\nClass distribution after upsampling:\")\n",
    "print(y_train_upsampled.value_counts())\n",
    "\n",
    "# Train a model on upsampled data\n",
    "upsampled_model = RandomForestClassifier(random_state=42)\n",
    "upsampled_model.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_upsampled = upsampled_model.predict(X_val)\n",
    "upsampled_f1 = f1_score(y_val, y_val_pred_upsampled)\n",
    "upsampled_auc = roc_auc_score(y_val, upsampled_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(\"\\nUpsampling model results:\")\n",
    "print(f\"F1 Score: {upsampled_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {upsampled_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3: Downsampling the Majority Class\n",
    "\n",
    "# Downsample the majority class (Exited=0)\n",
    "X_train_majority_downsampled, y_train_majority_downsampled = resample(\n",
    "    X_train_majority, \n",
    "    y_train_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(X_train_minority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "X_train_downsampled = pd.concat([X_train_majority_downsampled, X_train_minority])\n",
    "y_train_downsampled = pd.concat([y_train_majority_downsampled, y_train_minority])\n",
    "\n",
    "print(\"\\nClass distribution after downsampling:\")\n",
    "print(y_train_downsampled.value_counts())\n",
    "\n",
    "# Train a model on downsampled data\n",
    "downsampled_model = RandomForestClassifier(random_state=42)\n",
    "downsampled_model.fit(X_train_downsampled, y_train_downsampled)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_downsampled = downsampled_model.predict(X_val)\n",
    "downsampled_f1 = f1_score(y_val, y_val_pred_downsampled)\n",
    "downsampled_auc = roc_auc_score(y_val, downsampled_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(\"\\nDownsampling model results:\")\n",
    "print(f\"F1 Score: {downsampled_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {downsampled_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare all models\n",
    "models = {\n",
    "    'Baseline': {\n",
    "        'F1': baseline_f1,\n",
    "        'AUC': baseline_auc\n",
    "    },\n",
    "    'Class Weights': {\n",
    "        'F1': weighted_f1,\n",
    "        'AUC': weighted_auc\n",
    "    },\n",
    "    'Upsampling': {\n",
    "        'F1': upsampled_f1,\n",
    "        'AUC': upsampled_auc\n",
    "    },\n",
    "    'Downsampling': {\n",
    "        'F1': downsampled_f1,\n",
    "        'AUC': downsampled_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "comparison_df = pd.DataFrame(models).T\n",
    "comparison_df = comparison_df.sort_values('F1', ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Select the best model based on F1 score\n",
    "best_model_name = comparison_df.index[0]\n",
    "print(f\"\\nBest model based on F1 score: {best_model_name}\")\n",
    "\n",
    "# Select the appropriate model for hyperparameter tuning\n",
    "if best_model_name == 'Class Weights':\n",
    "    best_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    X_train_best, y_train_best = X_train, y_train\n",
    "elif best_model_name == 'Upsampling':\n",
    "    best_model = RandomForestClassifier(random_state=42)\n",
    "    X_train_best, y_train_best = X_train_upsampled, y_train_upsampled\n",
    "elif best_model_name == 'Downsampling':\n",
    "    best_model = RandomForestClassifier(random_state=42)\n",
    "    X_train_best, y_train_best = X_train_downsampled, y_train_downsampled\n",
    "else:\n",
    "    best_model = RandomForestClassifier(random_state=42)\n",
    "    X_train_best, y_train_best = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5.1: Executive Summary of Model Approaches\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXECUTIVE SUMMARY - MODEL APPROACH COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "BUSINESS CONTEXT:\n",
    "Our dataset shows 80% customer retention vs 20% churn, creating a prediction    challenge where standard models favor the majority class and poorly identify at-risk customers.\n",
    "\n",
    "APPROACH RESULTS:\n",
    "- Baseline Model: F1 Score {baseline_f1:.4f} - Inadequate churn detection due to class imbalance\n",
    "- Class Weights: F1 Score {weighted_f1:.4f} - Improved by penalizing misclassification of churners  \n",
    "- Upsampling: F1 Score {upsampled_f1:.4f} - Enhanced by creating synthetic churn examples for training\n",
    "- Downsampling: F1 Score {downsampled_f1:.4f} - Balanced by reducing non-churn training samples\n",
    "\n",
    "RECOMMENDATION:\n",
    "The {best_model_name} approach achieved our highest F1 score of {comparison_df.iloc[0]['F1']:.4f}, \n",
    "providing optimal churn identification for proactive customer retention strategies.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Hyperparameter Tuning for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"HYPERPARAMETER TUNING FOR ALL MODELS\")\n",
    "\n",
    "# Dictionary to store tuned models and their performance\n",
    "tuned_models_results = {}\n",
    "\n",
    "# 1. Tune Baseline Model\n",
    "print(\"\\n1. Tuning Baseline Model...\")\n",
    "baseline_model = RandomForestClassifier(random_state=42)\n",
    "baseline_grid = GridSearchCV(\n",
    "    estimator=baseline_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "baseline_grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate tuned baseline model\n",
    "baseline_tuned = baseline_grid.best_estimator_\n",
    "y_val_pred_baseline_tuned = baseline_tuned.predict(X_val)\n",
    "baseline_tuned_f1 = f1_score(y_val, y_val_pred_baseline_tuned)\n",
    "baseline_tuned_auc = roc_auc_score(y_val, baseline_tuned.predict_proba(X_val)[:, 1])\n",
    "\n",
    "tuned_models_results['Baseline'] = {\n",
    "    'model': baseline_tuned,\n",
    "    'best_params': baseline_grid.best_params_,\n",
    "    'cv_f1': baseline_grid.best_score_,\n",
    "    'val_f1': baseline_tuned_f1,\n",
    "    'val_auc': baseline_tuned_auc,\n",
    "    'training_data': (X_train, y_train)\n",
    "}\n",
    "\n",
    "print(f\"Best parameters: {baseline_grid.best_params_}\")\n",
    "print(f\"Best CV F1 score: {baseline_grid.best_score_:.4f}\")\n",
    "print(f\"Validation F1 score: {baseline_tuned_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tune Class Weights Model\n",
    "print(\"\\n2. Tuning Class Weights Model...\")\n",
    "# Modify param_grid to include class_weight\n",
    "param_grid_weighted = param_grid.copy()\n",
    "param_grid_weighted['class_weight'] = ['balanced']\n",
    "\n",
    "weighted_model = RandomForestClassifier(random_state=42)\n",
    "weighted_grid = GridSearchCV(\n",
    "    estimator=weighted_model,\n",
    "    param_grid=param_grid_weighted,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "weighted_grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate tuned weighted model\n",
    "weighted_tuned = weighted_grid.best_estimator_\n",
    "y_val_pred_weighted_tuned = weighted_tuned.predict(X_val)\n",
    "weighted_tuned_f1 = f1_score(y_val, y_val_pred_weighted_tuned)\n",
    "weighted_tuned_auc = roc_auc_score(y_val, weighted_tuned.predict_proba(X_val)[:, 1])\n",
    "\n",
    "tuned_models_results['Class Weights'] = {\n",
    "    'model': weighted_tuned,\n",
    "    'best_params': weighted_grid.best_params_,\n",
    "    'cv_f1': weighted_grid.best_score_,\n",
    "    'val_f1': weighted_tuned_f1,\n",
    "    'val_auc': weighted_tuned_auc,\n",
    "    'training_data': (X_train, y_train)\n",
    "}\n",
    "\n",
    "print(f\"Best parameters: {weighted_grid.best_params_}\")\n",
    "print(f\"Best CV F1 score: {weighted_grid.best_score_:.4f}\")\n",
    "print(f\"Validation F1 score: {weighted_tuned_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tune Upsampling Model\n",
    "print(\"\\n3. Tuning Upsampling Model...\")\n",
    "upsampling_model = RandomForestClassifier(random_state=42)\n",
    "upsampling_grid = GridSearchCV(\n",
    "    estimator=upsampling_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "upsampling_grid.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Evaluate tuned upsampling model\n",
    "upsampling_tuned = upsampling_grid.best_estimator_\n",
    "y_val_pred_upsampling_tuned = upsampling_tuned.predict(X_val)\n",
    "upsampling_tuned_f1 = f1_score(y_val, y_val_pred_upsampling_tuned)\n",
    "upsampling_tuned_auc = roc_auc_score(y_val, upsampling_tuned.predict_proba(X_val)[:, 1])\n",
    "\n",
    "tuned_models_results['Upsampling'] = {\n",
    "    'model': upsampling_tuned,\n",
    "    'best_params': upsampling_grid.best_params_,\n",
    "    'cv_f1': upsampling_grid.best_score_,\n",
    "    'val_f1': upsampling_tuned_f1,\n",
    "    'val_auc': upsampling_tuned_auc,\n",
    "    'training_data': (X_train_upsampled, y_train_upsampled)\n",
    "}\n",
    "\n",
    "print(f\"Best parameters: {upsampling_grid.best_params_}\")\n",
    "print(f\"Best CV F1 score: {upsampling_grid.best_score_:.4f}\")\n",
    "print(f\"Validation F1 score: {upsampling_tuned_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tune Downsampling Model\n",
    "print(\"\\n4. Tuning Downsampling Model...\")\n",
    "downsampling_model = RandomForestClassifier(random_state=42)\n",
    "downsampling_grid = GridSearchCV(\n",
    "    estimator=downsampling_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "downsampling_grid.fit(X_train_downsampled, y_train_downsampled)\n",
    "\n",
    "# Evaluate tuned downsampling model\n",
    "downsampling_tuned = downsampling_grid.best_estimator_\n",
    "y_val_pred_downsampling_tuned = downsampling_tuned.predict(X_val)\n",
    "downsampling_tuned_f1 = f1_score(y_val, y_val_pred_downsampling_tuned)\n",
    "downsampling_tuned_auc = roc_auc_score(y_val, downsampling_tuned.predict_proba(X_val)[:, 1])\n",
    "\n",
    "tuned_models_results['Downsampling'] = {\n",
    "    'model': downsampling_tuned,\n",
    "    'best_params': downsampling_grid.best_params_,\n",
    "    'cv_f1': downsampling_grid.best_score_,\n",
    "    'val_f1': downsampling_tuned_f1,\n",
    "    'val_auc': downsampling_tuned_auc,\n",
    "    'training_data': (X_train_downsampled, y_train_downsampled)\n",
    "}\n",
    "\n",
    "print(f\"Best parameters: {downsampling_grid.best_params_}\")\n",
    "print(f\"Best CV F1 score: {downsampling_grid.best_score_:.4f}\")\n",
    "print(f\"Validation F1 score: {downsampling_tuned_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.1  Compare All Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, results in tuned_models_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'CV_F1': results['cv_f1'],\n",
    "        'Validation_F1': results['val_f1'],\n",
    "        'Validation_AUC': results['val_auc']\n",
    "    })\n",
    "\n",
    "tuned_comparison_df = pd.DataFrame(comparison_data)\n",
    "tuned_comparison_df = tuned_comparison_df.sort_values('Validation_F1', ascending=False)\n",
    "print(\"\\nTuned Models Performance:\")\n",
    "print(tuned_comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Select the best model based on validation F1 score\n",
    "best_model_name = tuned_comparison_df.iloc[0]['Model']\n",
    "best_model_info = tuned_models_results[best_model_name]\n",
    "final_model = best_model_info['model']\n",
    "\n",
    "print(f\"\\n BEST MODEL: {best_model_name}\")\n",
    "print(f\"Validation F1 Score: {best_model_info['val_f1']:.4f}\")\n",
    "print(f\"Validation AUC Score: {best_model_info['val_auc']:.4f}\")\n",
    "print(f\"Best Parameters: {best_model_info['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.2 Final Model Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_final = final_model.predict(X_val)\n",
    "final_f1 = f1_score(y_val, y_val_pred_final)\n",
    "final_auc = roc_auc_score(y_val, final_model.predict_proba(X_val)[:, 1])\n",
    "\n",
    "print(f\"\\nFinal Model Performance on Validation Set:\")\n",
    "print(f\"F1 Score: {final_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {final_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_final))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.3 Executive Summary - Hyperparameter Tuning Results\n",
    "\n",
    "TUNING PROCESS:\n",
    "All four model approaches underwent comprehensive hyperparameter optimization using \n",
    "5-fold cross-validation to find optimal parameters for each approach.\n",
    "\n",
    "PARAMETER GRID TESTED:\n",
    "- n_estimators: [50, 100, 200]\n",
    "- max_depth: [None, 10, 20] \n",
    "- min_samples_split: [2, 5]\n",
    "- min_samples_leaf: [1, 2, 4]\n",
    "\n",
    "BEST PERFORMING MODEL: Class Weights\n",
    "- Validation F1 Score: 0.6380\n",
    "- Validation AUC Score: 0.8701\n",
    "- Optimal Parameters: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Final Testing On Hold Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final tuned model on the test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "final_f1 = f1_score(y_test, y_test_pred)\n",
    "final_auc = roc_auc_score(y_test, final_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "\n",
    "print(\"FINAL TEST RESULTS:\")\n",
    "\n",
    "print(f\"F1 Score: {final_f1:.4f}\")\n",
    "print(f\"AUC-ROC: {final_auc:.4f}\")\n",
    "print(f\"Project Requirement: F1 >= 0.59\")\n",
    "print(f\"Requirement Met: {'YES' if final_f1 >= 0.59 else 'NO'}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Visualizations and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, final_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {final_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Customer Churn Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Feature Importance\n",
    "plt.subplot(1, 2, 2)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': final_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.barh(range(len(feature_importance)), feature_importance['Importance'])\n",
    "plt.yticks(range(len(feature_importance)), feature_importance['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance in Customer Churn Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8.1: Buisness Insights from Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_features = feature_importance.head(5)\n",
    "print(\"\\nKey Drivers of Customer Churn:\")\n",
    "for idx, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['Feature']}: {row['Importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIONABLE INSIGHTS:\n",
    "Based on the top features identified by our Baseline model, the business\n",
    "should focus retention efforts on customers with characteristics related to the\n",
    "most important features shown above.\n",
    "\n",
    "# MODEL PERFORMANCE SUMMARY:\n",
    "- Final Test F1 Score: 0.5813\n",
    "- Final Test AUC Score: 0.8597\n",
    "- Model successfully does not meet project requirements\n",
    "- Model approach: Baseline with optimized hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Final Summary and Conclusion\n",
    "\n",
    "# PROJECT SUMMARY - BETA BANK CUSTOMER CHURN PREDICTION\n",
    "\n",
    "1. DATASET OVERVIEW:\n",
    "   - Total customers: 10000\n",
    "   - Features: 11\n",
    "   - Class distribution: 7963 stayed, 2037 exited\n",
    "   - Imbalance ratio: 3.9:1\n",
    "\n",
    "2. MODEL PERFORMANCE COMPARISON:\n",
    "   - Baseline: F1=0.5842, AUC=0.8563\n",
    "   - Class Weights: F1=0.5818, AUC=0.8591\n",
    "   - Upsampling: F1=0.6176, AUC=0.8587\n",
    "   - Downsampling: F1=0.5937, AUC=0.8531\n",
    "\n",
    "3. BEST MODEL:\n",
    "   - Approach: Class Weights\n",
    "   - Final F1 Score: 0.6311\n",
    "   - Final AUC-ROC: 0.8666\n",
    "   - Project Requirement (F1 >= 0.59): âœ“ MET\n",
    "\n",
    "4. KEY INSIGHTS:\n",
    "   - Most important feature: Age\n",
    "   - Class imbalance significantly affected baseline model performance\n",
    "   - Class Weights approach was most effective for this dataset\n",
    "\n",
    "5. BUSINESS RECOMMENDATIONS:\n",
    "   - Focus retention efforts on customers with high churn probability\n",
    "   - Monitor key features: Age, NumOfProducts, Balance\n",
    "   - Implement proactive customer engagement strategies\n",
    "   - Regular model retraining recommended as new data becomes available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightblue; color:darkblue\">\n",
    "\n",
    "All cells have be updated. Thank you for your evaluation! </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
